{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06188412-f34c-415e-b59c-75c5a8a9abb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96a1f9aa-ec9f-4d51-bfe3-a7e2dabef95d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KAGGLE_USERNAME\"] = \"sumeshmajee\"\n",
    "os.environ[\"KAGGLE_KEY\"] = \"KGAT_e8b80db00821caf2bf9e6a6690d004c4\"\n",
    "\n",
    "print(\"Kaggle credentials configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c469599-22a4-4f08-9b9c-1ccaa92c60b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE SCHEMA IF NOT EXISTS workspace.ecommerce\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58aaab97-a86b-40f5-826b-9c89c75abe89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE VOLUME IF NOT EXISTS workspace.ecommerce.ecommerce_data\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89e6da2b-07df-4576-bec3-910cbbc33bc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "cd /Volumes/workspace/ecommerce/ecommerce_data\n",
    "kaggle datasets download -d mkechinov/ecommerce-behavior-data-from-multi-category-store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42e42527-577c-44e6-a00e-5776b35e92da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "cd /Volumes/workspace/ecommerce/ecommerce_data\n",
    "unzip -o ecommerce-behavior-data-from-multi-category-store.zip\n",
    "ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae177ac5-1444-4580-97a5-1060d28ee67d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "cd /Volumes/workspace/ecommerce/ecommerce_data\n",
    "rm -f ecommerce-behavior-data-from-multi-category-store.zip\n",
    "ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e522143-8468-4070-bbd2-56cc8f10242c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c206650-31cb-4ec1-a9c9-071d4704069c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_n = spark.read.csv(\"/Volumes/workspace/ecommerce/ecommerce_data/2019-Nov.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f664cc80-ef4d-44d3-9164-8845e9fd2326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/Volumes/workspace/ecommerce/ecommerce_data/2019-Oct.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "bff3746c-4870-40b2-b89e-65dd888e69b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"October 2019 - Total Events: {df.count():,}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCHEMA:\")\n",
    "print(\"=\"*60)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8ebe903b-2b33-46fc-8362-8e866558d08e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE DATA (First 5 rows):\")\n",
    "print(\"=\"*60)\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98f21d15-ef86-4c72-b939-7bc02cbeac71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Day 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "117d8e4f-4db6-4498-afe7-b91eafb9435a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create simple DataFrame\n",
    "data = [(\"iPhone\", 999), (\"Samsung\", 799), (\"MacBook\", 1299)]\n",
    "df = spark.createDataFrame(data, [\"product\", \"price\"])\n",
    "df.show()\n",
    "\n",
    "# Filter expensive products\n",
    "df.filter(df.price > 1000).show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6a397f2b-25c1-403a-afa9-8336bac13573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# 1. Enter your token securely when prompted (It won't be saved in the file)\n",
    "print(\"Paste your GitHub Token below and hit Enter:\")\n",
    "GITHUB_TOKEN = getpass.getpass()\n",
    "\n",
    "# --- Everything else stays the same ---\n",
    "USERNAME = \"SRMajee\"\n",
    "REPO_NAME = \"DataBricks_Challenge\"\n",
    "auth_url = f\"https://{GITHUB_TOKEN}@github.com/{USERNAME}/{REPO_NAME}.git\"\n",
    "\n",
    "# 2. Update Remote\n",
    "!git -c safe.directory='*' remote set-url origin {auth_url}\n",
    "\n",
    "# 3. Add files (Now the file doesn't contain the secret!)\n",
    "!git -c safe.directory='*' add .\n",
    "\n",
    "# 4. Commit\n",
    "!git -c safe.directory='*' commit -m \"Secure push using getpass\"\n",
    "\n",
    "# 5. Push\n",
    "!git -c safe.directory='*' push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c3845c6-9e0f-43a7-a419-57632378ffd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Day 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6708ae5-2d56-4597-ab95-921a16322662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "events = spark.read.csv(\"/Volumes/workspace/ecommerce/ecommerce_data/2019-Oct.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "930e9ca0-c767-4b72-a073-39b0754cf3fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Basic operations\n",
    "events.select(\"event_time\", \"product_id\", \"price\").show(10)\n",
    "events.filter(\"price > 100\").count()\n",
    "events.groupBy(\"event_time\").count().show()\n",
    "top_brands = events.groupBy(\"brand\").count().orderBy(\"count\", ascending=False).limit(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d059cad0-6a7b-47d7-917a-0216c0374544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# 1. UNDO the last commit (so we can save it again with the private email)\n",
    "!git -c safe.directory='*' reset --soft HEAD~1\n",
    "\n",
    "# 2. SET PRIVATE IDENTITY (Using your GitHub no-reply alias)\n",
    "# This format (username@users.noreply.github.com) hides your real email\n",
    "os.environ[\"GIT_AUTHOR_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_AUTHOR_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "os.environ[\"GIT_COMMITTER_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_COMMITTER_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "\n",
    "# 3. ASK FOR TOKEN\n",
    "print(\"Paste your GitHub Token below and hit Enter:\")\n",
    "GITHUB_TOKEN = getpass.getpass()\n",
    "\n",
    "# 4. PREPARE URL\n",
    "USERNAME = \"SRMajee\"\n",
    "REPO_NAME = \"DataBricks_Challenge\"\n",
    "auth_url = f\"https://{GITHUB_TOKEN}@github.com/{USERNAME}/{REPO_NAME}.git\"\n",
    "\n",
    "# 5. COMMIT & PUSH\n",
    "!git -c safe.directory='*' remote set-url origin {auth_url}\n",
    "!git -c safe.directory='*' commit -m \"Day 2\"\n",
    "!git -c safe.directory='*' push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1525c337-4b41-4933-9454-186ae49c5c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Day 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "caccce9a-00b5-43f7-a860-619a5c794da0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Top 5 products by revenue\n",
    "revenue = (\n",
    "    events.filter(F.col(\"event_type\") == \"purchase\")\n",
    "    .groupBy(\"product_id\", \"product_id\")\n",
    "    .agg(F.sum(\"price\").alias(\"revenue\"))\n",
    "    .orderBy(F.desc(\"revenue\"))\n",
    "    .limit(5)\n",
    ")\n",
    "\n",
    "# Running total per user\n",
    "window = Window.partitionBy(\"user_id\").orderBy(\"event_time\")\n",
    "events_with_cumulative = events.withColumn(\n",
    "    \"cumulative_events\",\n",
    "    F.count(\"*\").over(window)\n",
    ")\n",
    "\n",
    "# Conversion rate by category (replace pivot with conditional aggregation)\n",
    "conversion = (\n",
    "    events.groupBy(\"category_code\")\n",
    "    .agg(\n",
    "        F.sum(F.when(F.col(\"event_type\") == \"purchase\", 1).otherwise(0)).alias(\"purchase\"),\n",
    "        F.sum(F.when(F.col(\"event_type\") == \"view\", 1).otherwise(0)).alias(\"view\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"conversion_rate\",\n",
    "        F.col(\"purchase\") / F.col(\"view\") * 100\n",
    "    )\n",
    ")\n",
    "\n",
    "display(revenue)\n",
    "display(events_with_cumulative)\n",
    "display(conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "99f749ee-cf05-45c4-a2e5-5d73cd6b2edd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# 1. UNDO the last commit (so we can try the process again from a clean state)\n",
    "# We do this to ensure the commit and pull happen in the right order.\n",
    "!git -c safe.directory='*' reset --soft HEAD~1\n",
    "\n",
    "# 2. SET PRIVATE IDENTITY\n",
    "os.environ[\"GIT_AUTHOR_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_AUTHOR_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "os.environ[\"GIT_COMMITTER_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_COMMITTER_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "\n",
    "# 3. ASK FOR TOKEN\n",
    "print(\"Paste your GitHub Token below and hit Enter:\")\n",
    "GITHUB_TOKEN = getpass.getpass()\n",
    "\n",
    "# 4. PREPARE URL\n",
    "USERNAME = \"SRMajee\"\n",
    "REPO_NAME = \"DataBricks_Challenge\"\n",
    "auth_url = f\"https://{GITHUB_TOKEN}@github.com/{USERNAME}/{REPO_NAME}.git\"\n",
    "\n",
    "# 5. COMMIT, PULL (WITH FIX), THEN PUSH\n",
    "!git -c safe.directory='*' remote set-url origin {auth_url}\n",
    "!git -c safe.directory='*' commit -m \"Day 3\"\n",
    "\n",
    "# FIX IS HERE: We added '--no-rebase' to tell Git to use the default merge strategy\n",
    "!git -c safe.directory='*' pull origin main --no-rebase --no-edit\n",
    "\n",
    "# NOW PUSH\n",
    "!git -c safe.directory='*' push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69a944f8-3241-4e68-b347-eb5fe6511805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Day 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ebe6e286-6250-47c1-87d9-0469d9b1419f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write Delta table and schema note"
    }
   },
   "outputs": [],
   "source": [
    "# Use a Unity Catalog volume directory path, not a file\n",
    "volume_path = \"/Volumes/workspace/ecommerce/ecommerce_data/events_delta\"\n",
    "\n",
    "# Write DataFrame as Delta table to the directory\n",
    "events.write.format(\"delta\").mode(\"overwrite\").save(volume_path)\n",
    "\n",
    "# Register the Delta table in Unity Catalog\n",
    "events.write.format(\"delta\").saveAsTable(\"workspace.ecommerce.events_table\")\n",
    "\n",
    "# SQL approach to create a managed Delta table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE workspace.ecommerce.events_delta\n",
    "    USING DELTA\n",
    "    AS SELECT * FROM workspace.ecommerce.events_table\n",
    "\"\"\")\n",
    "\n",
    "# Test schema enforcement\n",
    "try:\n",
    "    wrong_schema = spark.createDataFrame([(\"a\",\"b\",\"c\")], [\"x\",\"y\",\"z\"])\n",
    "    wrong_schema.write.format(\"delta\").mode(\"append\").save(volume_path)\n",
    "except Exception as e:\n",
    "    print(f\"Schema enforcement: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "df9ea388-4a94-4f08-ba59-501400ba2637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# # 1. UNDO the last commit (so we can try the process again from a clean state)\n",
    "# # We do this to ensure the commit and pull happen in the right order.\n",
    "# !git -c safe.directory='*' reset --soft HEAD~1\n",
    "\n",
    "# 2. SET PRIVATE IDENTITY\n",
    "os.environ[\"GIT_AUTHOR_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_AUTHOR_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "os.environ[\"GIT_COMMITTER_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_COMMITTER_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "\n",
    "# 3. ASK FOR TOKEN\n",
    "print(\"Paste your GitHub Token below and hit Enter:\")\n",
    "GITHUB_TOKEN = getpass.getpass()\n",
    "\n",
    "# 4. PREPARE URL\n",
    "USERNAME = \"SRMajee\"\n",
    "REPO_NAME = \"DataBricks_Challenge\"\n",
    "auth_url = f\"https://{GITHUB_TOKEN}@github.com/{USERNAME}/{REPO_NAME}.git\"\n",
    "\n",
    "# 5. ADD, COMMIT, PULL, THEN PUSH\n",
    "!git -c safe.directory='*' remote set-url origin {auth_url}\n",
    "\n",
    "# --- FIX: ADD FILES FIRST ---\n",
    "!git -c safe.directory='*' add . \n",
    "# ----------------------------\n",
    "\n",
    "!git -c safe.directory='*' commit -m \"Day 4\"\n",
    "\n",
    "# Pull just in case there are remote changes\n",
    "!git -c safe.directory='*' pull origin main --no-rebase --no-edit\n",
    "\n",
    "# NOW PUSH\n",
    "!git -c safe.directory='*' push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34e6ddac-564f-445e-b50c-613e5e625982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Day 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1e898c0e-d47c-480a-8edd-8d0de0104439",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 27"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "volume_path = \"/Volumes/workspace/ecommerce/ecommerce_data/events_delta\"\n",
    "# MERGE for incremental updates\n",
    "deltaTable = DeltaTable.forPath(spark, volume_path)\n",
    "updates = spark.read.csv(\"/Volumes/workspace/ecommerce/ecommerce_data/2019-Nov.csv\", header=True, inferSchema=True)\n",
    "\n",
    "deltaTable.alias(\"t\").merge(\n",
    "    updates.alias(\"s\"),\n",
    "    \"t.user_session = s.user_session AND t.event_time = s.event_time\"\n",
    ").whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()\n",
    "\n",
    "# Time travel\n",
    "v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(volume_path)\n",
    "yesterday = spark.read.format(\"delta\") \\\n",
    "    .option(\"timestampAsOf\", \"2024-01-01\").load(volume_path)\n",
    "\n",
    "# Optimize\n",
    "spark.sql(\"OPTIMIZE events_table ZORDER BY (event_type, user_id)\")\n",
    "spark.sql(\"VACUUM events_table RETAIN 168 HOURS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "911f6ce1-d173-477c-8eb1-def301a1a2be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# # 1. UNDO the last commit (so we can try the process again from a clean state)\n",
    "# # We do this to ensure the commit and pull happen in the right order.\n",
    "# !git -c safe.directory='*' reset --soft HEAD~1\n",
    "\n",
    "# 2. SET PRIVATE IDENTITY\n",
    "os.environ[\"GIT_AUTHOR_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_AUTHOR_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "os.environ[\"GIT_COMMITTER_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_COMMITTER_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "\n",
    "# 3. ASK FOR TOKEN\n",
    "print(\"Paste your GitHub Token below and hit Enter:\")\n",
    "GITHUB_TOKEN = getpass.getpass()\n",
    "\n",
    "# 4. PREPARE URL\n",
    "USERNAME = \"SRMajee\"\n",
    "REPO_NAME = \"DataBricks_Challenge\"\n",
    "auth_url = f\"https://{GITHUB_TOKEN}@github.com/{USERNAME}/{REPO_NAME}.git\"\n",
    "\n",
    "# 5. ADD, COMMIT, PULL, THEN PUSH\n",
    "!git -c safe.directory='*' remote set-url origin {auth_url}\n",
    "\n",
    "# --- FIX: ADD FILES FIRST ---\n",
    "!git -c safe.directory='*' add . \n",
    "# ----------------------------\n",
    "\n",
    "!git -c safe.directory='*' commit -m \"Day 5\"\n",
    "\n",
    "# Pull just in case there are remote changes\n",
    "!git -c safe.directory='*' pull origin main --no-rebase --no-edit\n",
    "\n",
    "# NOW PUSH\n",
    "!git -c safe.directory='*' push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53a715f1-b14a-4d46-b813-5bd4d76a52d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Day 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2e9e943-4e94-4d6f-b4a0-035a7b08d773",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 30"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# BRONZE: Raw ingestion\n",
    "raw = spark.read.csv(\"/Volumes/workspace/ecommerce/ecommerce_data/2019-Nov.csv\", header=True, inferSchema=True)\n",
    "raw = raw.withColumn(\"price\", F.col(\"price\").cast(\"double\"))\n",
    "raw.withColumn(\"ingestion_ts\", F.current_timestamp()) \\\n",
    "   .write.format(\"delta\").mode(\"overwrite\").save(\"/Volumes/workspace/ecommerce/ecommerce_data/bronze_events\")\n",
    "\n",
    "# SILVER: Cleaned data\n",
    "bronze = spark.read.format(\"delta\").load(\"/Volumes/workspace/ecommerce/ecommerce_data/bronze_events\")\n",
    "silver = bronze.filter(F.col(\"price\") > 0) \\\n",
    "    .filter(F.col(\"price\") < 10000) \\\n",
    "    .dropDuplicates([\"user_session\", \"event_time\"]) \\\n",
    "    .withColumn(\"event_date\", F.to_date(\"event_time\")) \\\n",
    "    .withColumn(\"price_tier\",\n",
    "        F.when(F.col(\"price\") < 10, \"budget\")\n",
    "         .when(F.col(\"price\") < 50, \"mid\")\n",
    "         .otherwise(\"premium\"))\n",
    "silver.write.format(\"delta\").mode(\"overwrite\").save(\"/Volumes/workspace/ecommerce/ecommerce_data/silver_events\")\n",
    "\n",
    "# GOLD: Aggregates\n",
    "silver = spark.read.format(\"delta\").load(\"/Volumes/workspace/ecommerce/ecommerce_data/silver_events\")\n",
    "product_perf = silver.groupBy(\"product_id\") \\\n",
    "    .agg(\n",
    "        F.countDistinct(F.when(F.col(\"event_type\")==\"view\", \"user_id\")).alias(\"views\"),\n",
    "        F.countDistinct(F.when(F.col(\"event_type\")==\"purchase\", \"user_id\")).alias(\"purchases\"),\n",
    "        F.sum(F.when(F.col(\"event_type\")==\"purchase\", F.col(\"price\"))).alias(\"revenue\")\n",
    "    ).withColumn(\n",
    "        \"conversion_rate\",\n",
    "        F.when(F.col(\"views\") > 0, F.col(\"purchases\")/F.col(\"views\")*100).otherwise(None)\n",
    "    )\n",
    "product_perf.write.format(\"delta\").mode(\"overwrite\").save(\"/Volumes/workspace/ecommerce/ecommerce_data/gold_products\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6c9143be-af83-4cca-85ad-d20aa2d386fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# # 1. UNDO the last commit (so we can try the process again from a clean state)\n",
    "# # We do this to ensure the commit and pull happen in the right order.\n",
    "# !git -c safe.directory='*' reset --soft HEAD~1\n",
    "\n",
    "# 2. SET PRIVATE IDENTITY\n",
    "os.environ[\"GIT_AUTHOR_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_AUTHOR_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "os.environ[\"GIT_COMMITTER_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_COMMITTER_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "\n",
    "# 3. ASK FOR TOKEN\n",
    "print(\"Paste your GitHub Token below and hit Enter:\")\n",
    "GITHUB_TOKEN = getpass.getpass()\n",
    "\n",
    "# 4. PREPARE URL\n",
    "USERNAME = \"SRMajee\"\n",
    "REPO_NAME = \"DataBricks_Challenge\"\n",
    "auth_url = f\"https://{GITHUB_TOKEN}@github.com/{USERNAME}/{REPO_NAME}.git\"\n",
    "\n",
    "# 5. ADD, COMMIT, PULL, THEN PUSH\n",
    "!git -c safe.directory='*' remote set-url origin {auth_url}\n",
    "\n",
    "# --- FIX: ADD FILES FIRST ---\n",
    "!git -c safe.directory='*' add . \n",
    "# ----------------------------\n",
    "\n",
    "!git -c safe.directory='*' commit -m \"Day 6\"\n",
    "\n",
    "# Pull just in case there are remote changes\n",
    "!git -c safe.directory='*' pull origin main --no-rebase --no-edit\n",
    "\n",
    "# NOW PUSH\n",
    "!git -c safe.directory='*' push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "816c4db1-469d-4c2c-8e94-28025940982f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Day 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3af98d26-fb7d-4ede-a580-aa12ffca5771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"source_path\", \"/Volumes/workspace/ecommerce/ecommerce_data/bronze_events\")\n",
    "dbutils.widgets.dropdown(\"layer\", \"bronze\", [\"bronze\",\"silver\",\"gold\"])\n",
    "\n",
    "# Use parameters\n",
    "source = dbutils.widgets.get(\"source_path\")\n",
    "layer = dbutils.widgets.get(\"layer\")\n",
    "\n",
    "def run_layer(layer_name):\n",
    "    if layer_name == \"bronze\":\n",
    "        # Bronze logic\n",
    "        df = spark.read.csv(source, header=True, inferSchema=True)\n",
    "        df = df.withColumn(\"price\", F.col(\"price\").cast(\"double\"))\n",
    "        df = df.withColumn(\"ingestion_ts\", F.current_timestamp())\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").save(\"/Volumes/workspace/ecommerce/ecommerce_data/bronze_events\")\n",
    "    elif layer_name == \"silver\":\n",
    "        # Silver logic\n",
    "        bronze = spark.read.format(\"delta\").load(\"/Volumes/workspace/ecommerce/ecommerce_data/bronze_events\")\n",
    "        silver = bronze.filter(F.col(\"price\") > 0) \\\n",
    "            .filter(F.col(\"price\") < 10000) \\\n",
    "            .dropDuplicates([\"user_session\", \"event_time\"]) \\\n",
    "            .withColumn(\"event_date\", F.to_date(\"event_time\")) \\\n",
    "            .withColumn(\"price_tier\",\n",
    "                F.when(F.col(\"price\") < 10, \"budget\")\n",
    "                 .when(F.col(\"price\") < 50, \"mid\")\n",
    "                 .otherwise(\"premium\"))\n",
    "        silver.write.format(\"delta\").mode(\"overwrite\").save(\"/Volumes/workspace/ecommerce/ecommerce_data/silver_events\")\n",
    "    elif layer_name == \"gold\":\n",
    "        # Gold logic\n",
    "        silver = spark.read.format(\"delta\").load(\"/Volumes/workspace/ecommerce/ecommerce_data/silver_events\")\n",
    "        product_perf = silver.groupBy(\"product_id\") \\\n",
    "            .agg(\n",
    "                F.countDistinct(F.when(F.col(\"event_type\")==\"view\", \"user_id\")).alias(\"views\"),\n",
    "                F.countDistinct(F.when(F.col(\"event_type\")==\"purchase\", \"user_id\")).alias(\"purchases\"),\n",
    "                F.sum(F.when(F.col(\"event_type\")==\"purchase\", F.col(\"price\"))).alias(\"revenue\")\n",
    "            ).withColumn(\n",
    "                \"conversion_rate\",\n",
    "                F.when(F.col(\"views\") > 0, F.col(\"purchases\")/F.col(\"views\")*100).otherwise(None)\n",
    "            )\n",
    "        product_perf.write.format(\"delta\").mode(\"overwrite\").save(\"/Volumes/workspace/ecommerce/ecommerce_data/gold_products\")\n",
    "\n",
    "# To schedule this notebook to run daily at 2 AM:\n",
    "# 1. In Databricks, go to \"Jobs\" > \"Create Job\".\n",
    "# 2. Select this notebook as the task.\n",
    "# 3. Set the schedule to \"Daily\" and choose \"2:00 AM\" as the start time.\n",
    "# 4. Save the job.\n",
    "\n",
    "# UI: Create Databricks Job\n",
    "# Task 1: bronze_layer (notebook)\n",
    "# Task 2: silver_layer (notebook, depends on Task 1)\n",
    "# Task 3: gold_layer (notebook, depends on Task 2)\n",
    "# Schedule: Daily at 2 AM\n",
    "# Use the Databricks Jobs UI to configure these tasks and dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c348f210-5c27-49f3-aca1-8dd398e724ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# # 1. UNDO the last commit (so we can try the process again from a clean state)\n",
    "# # We do this to ensure the commit and pull happen in the right order.\n",
    "# !git -c safe.directory='*' reset --soft HEAD~1\n",
    "\n",
    "# 2. SET PRIVATE IDENTITY\n",
    "os.environ[\"GIT_AUTHOR_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_AUTHOR_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "os.environ[\"GIT_COMMITTER_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_COMMITTER_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "\n",
    "# 3. ASK FOR TOKEN\n",
    "print(\"Paste your GitHub Token below and hit Enter:\")\n",
    "GITHUB_TOKEN = getpass.getpass()\n",
    "\n",
    "# 4. PREPARE URL\n",
    "USERNAME = \"SRMajee\"\n",
    "REPO_NAME = \"DataBricks_Challenge\"\n",
    "auth_url = f\"https://{GITHUB_TOKEN}@github.com/{USERNAME}/{REPO_NAME}.git\"\n",
    "\n",
    "# 5. ADD, COMMIT, PULL, THEN PUSH\n",
    "!git -c safe.directory='*' remote set-url origin {auth_url}\n",
    "\n",
    "# --- FIX: ADD FILES FIRST ---\n",
    "!git -c safe.directory='*' add . \n",
    "# ----------------------------\n",
    "\n",
    "!git -c safe.directory='*' commit -m \"Day 7\"\n",
    "\n",
    "# Pull just in case there are remote changes\n",
    "!git -c safe.directory='*' pull origin main --no-rebase --no-edit\n",
    "\n",
    "# NOW PUSH\n",
    "!git -c safe.directory='*' push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2180e689-87ab-41c2-a8ef-e43f1c3d7353",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Day 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73a98f0c-f940-439c-b298-66b52921ceb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS gold.products; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aafc4cb1-e475-49ff-8dde-ac8ac1a432d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create structure\n",
    "CREATE CATALOG IF NOT EXISTS ecommerce;\n",
    "USE CATALOG ecommerce;\n",
    "CREATE SCHEMA IF NOT EXISTS bronze;\n",
    "CREATE SCHEMA IF NOT EXISTS silver;\n",
    "CREATE SCHEMA IF NOT EXISTS gold;\n",
    "\n",
    "-- Register tables as MANAGED (no LOCATION clause)\n",
    "CREATE TABLE IF NOT EXISTS bronze.events \n",
    "USING DELTA\n",
    "AS\n",
    "SELECT * FROM delta.`/Volumes/workspace/ecommerce/ecommerce_data/bronze_events`;\n",
    "CREATE TABLE IF NOT EXISTS silver.events \n",
    "USING DELTA \n",
    "AS\n",
    "SELECT * FROM delta.`/Volumes/workspace/ecommerce/ecommerce_data/silver_events`;\n",
    "CREATE OR REPLACE TABLE gold.products\n",
    "USING DELTA\n",
    "AS\n",
    "SELECT * FROM delta.`/Volumes/workspace/ecommerce/ecommerce_data/gold_products`;\n",
    "-- Controlled view\n",
    "CREATE OR REPLACE VIEW gold.top_products AS\n",
    "SELECT product_id, revenue, conversion_rate\n",
    "FROM gold.products\n",
    "WHERE purchases > 10\n",
    "ORDER BY revenue DESC LIMIT 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "90a24644-0aa4-4101-b3d6-855fd7e8c910",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# # 1. UNDO the last commit (so we can try the process again from a clean state)\n",
    "# # We do this to ensure the commit and pull happen in the right order.\n",
    "# !git -c safe.directory='*' reset --soft HEAD~1\n",
    "\n",
    "# 2. SET PRIVATE IDENTITY\n",
    "os.environ[\"GIT_AUTHOR_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_AUTHOR_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "os.environ[\"GIT_COMMITTER_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_COMMITTER_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "\n",
    "# 3. ASK FOR TOKEN\n",
    "print(\"Paste your GitHub Token below and hit Enter:\")\n",
    "GITHUB_TOKEN = getpass.getpass()\n",
    "\n",
    "# 4. PREPARE URL\n",
    "USERNAME = \"SRMajee\"\n",
    "REPO_NAME = \"DataBricks_Challenge\"\n",
    "auth_url = f\"https://{GITHUB_TOKEN}@github.com/{USERNAME}/{REPO_NAME}.git\"\n",
    "\n",
    "# 5. ADD, COMMIT, PULL, THEN PUSH\n",
    "!git -c safe.directory='*' remote set-url origin {auth_url}\n",
    "\n",
    "# --- FIX: ADD FILES FIRST ---\n",
    "!git -c safe.directory='*' add . \n",
    "# ----------------------------\n",
    "\n",
    "!git -c safe.directory='*' commit -m \"Day 8\"\n",
    "\n",
    "# Pull just in case there are remote changes\n",
    "!git -c safe.directory='*' pull origin main --no-rebase --no-edit\n",
    "\n",
    "# NOW PUSH\n",
    "!git -c safe.directory='*' push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6a372c6-1cb9-4299-9bbc-e61c308f69cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Day 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0b3c925-e2bd-4170-a8a2-f6786634e3b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 1. Overall Performance Metrics (Replacing Category Funnel)\n",
    "-- Calculates the total funnel across all products\n",
    "SELECT\n",
    "  SUM(views) as total_views,\n",
    "  SUM(purchases) as total_purchases,\n",
    "  SUM(revenue) as total_revenue,\n",
    "  ROUND(SUM(purchases) * 100.0 / SUM(views), 2) as global_conversion_rate\n",
    "FROM gold.products;\n",
    "\n",
    "-- 2. Top 10 Revenue Generators (Replacing Daily Revenue)\n",
    "-- Focuses on top performing products since we don't have dates\n",
    "SELECT\n",
    "  product_id,\n",
    "  revenue,\n",
    "  purchases,\n",
    "  views\n",
    "FROM gold.products\n",
    "ORDER BY revenue DESC\n",
    "LIMIT 10;\n",
    "\n",
    "-- 3. Top Converting Products (High Efficiency)\n",
    "-- Shows products effectively converting views to sales (filtered for noise)\n",
    "SELECT\n",
    "  product_id,\n",
    "  views,\n",
    "  purchases,\n",
    "  conversion_rate\n",
    "FROM gold.products\n",
    "WHERE views > 50  -- Filter to avoid 1 view / 1 purchase anomalies\n",
    "ORDER BY conversion_rate DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "23c68507-9520-429b-9969-071a4080b230",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# # 1. UNDO the last commit (so we can try the process again from a clean state)\n",
    "# # We do this to ensure the commit and pull happen in the right order.\n",
    "# !git -c safe.directory='*' reset --soft HEAD~1\n",
    "\n",
    "# 2. SET PRIVATE IDENTITY\n",
    "os.environ[\"GIT_AUTHOR_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_AUTHOR_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "os.environ[\"GIT_COMMITTER_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_COMMITTER_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "\n",
    "# 3. ASK FOR TOKEN\n",
    "print(\"Paste your GitHub Token below and hit Enter:\")\n",
    "GITHUB_TOKEN = getpass.getpass()\n",
    "\n",
    "# 4. PREPARE URL\n",
    "USERNAME = \"SRMajee\"\n",
    "REPO_NAME = \"DataBricks_Challenge\"\n",
    "auth_url = f\"https://{GITHUB_TOKEN}@github.com/{USERNAME}/{REPO_NAME}.git\"\n",
    "\n",
    "# 5. ADD, COMMIT, PULL, THEN PUSH\n",
    "!git -c safe.directory='*' remote set-url origin {auth_url}\n",
    "\n",
    "# --- FIX: ADD FILES FIRST ---\n",
    "!git -c safe.directory='*' add . \n",
    "# ----------------------------\n",
    "\n",
    "!git -c safe.directory='*' commit -m \"Day 9\"\n",
    "\n",
    "# Pull just in case there are remote changes\n",
    "!git -c safe.directory='*' pull origin main --no-rebase --no-edit\n",
    "\n",
    "# NOW PUSH\n",
    "!git -c safe.directory='*' push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41ea414e-92f7-463b-b88e-d68b8fbcec3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Day 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1649d4e0-580b-42b0-b148-46b4eef73bbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import time\n",
    "\n",
    "# 1. Ensure context\n",
    "spark.sql(\"USE CATALOG ecommerce\")\n",
    "spark.sql(\"USE SCHEMA gold\")\n",
    "\n",
    "# 2. Setup: Create the table for optimization (if not exists)\n",
    "spark.sql(\"\"\"\n",
    "  CREATE OR REPLACE TABLE gold.products_opt\n",
    "  USING DELTA\n",
    "  AS SELECT * FROM gold.products\n",
    "\"\"\")\n",
    "\n",
    "# 3. Optimize\n",
    "# Serverless handles file management, but ZORDER is still useful for layout\n",
    "print(\"--- Optimizing (Z-Ordering) ---\")\n",
    "spark.sql(\"OPTIMIZE gold.products_opt ZORDER BY (product_id, revenue)\")\n",
    "\n",
    "# 4. Get a valid ID for benchmarking (so we don't get Count: 0)\n",
    "# We fetch one existing product_id to test with\n",
    "valid_id_row = spark.sql(\"SELECT product_id FROM gold.products_opt LIMIT 1\").collect()\n",
    "\n",
    "if valid_id_row:\n",
    "    target_id = valid_id_row[0]['product_id']\n",
    "    print(f\"\\n--- Benchmarking for Product ID: {target_id} ---\")\n",
    "\n",
    "    start = time.time()\n",
    "    count = spark.sql(f\"SELECT * FROM gold.products_opt WHERE product_id = {target_id}\").count()\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"✅ Records found: {count}\")\n",
    "    print(f\"⏱️ Time taken: {end - start:.4f}s\")\n",
    "else:\n",
    "    print(\"⚠️ Table is empty. Cannot run benchmark.\")\n",
    "\n",
    "# Note: .cache() is removed because Serverless Compute caches hot data automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67da93bb-18d1-434a-8ece-162ab95ddcbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# # 1. UNDO the last commit (so we can try the process again from a clean state)\n",
    "# # We do this to ensure the commit and pull happen in the right order.\n",
    "# !git -c safe.directory='*' reset --soft HEAD~1\n",
    "\n",
    "# 2. SET PRIVATE IDENTITY\n",
    "os.environ[\"GIT_AUTHOR_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_AUTHOR_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "os.environ[\"GIT_COMMITTER_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_COMMITTER_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "\n",
    "# 3. ASK FOR TOKEN\n",
    "print(\"Paste your GitHub Token below and hit Enter:\")\n",
    "GITHUB_TOKEN = getpass.getpass()\n",
    "\n",
    "# 4. PREPARE URL\n",
    "USERNAME = \"SRMajee\"\n",
    "REPO_NAME = \"DataBricks_Challenge\"\n",
    "auth_url = f\"https://{GITHUB_TOKEN}@github.com/{USERNAME}/{REPO_NAME}.git\"\n",
    "\n",
    "# 5. ADD, COMMIT, PULL, THEN PUSH\n",
    "!git -c safe.directory='*' remote set-url origin {auth_url}\n",
    "\n",
    "# --- FIX: ADD FILES FIRST ---\n",
    "!git -c safe.directory='*' add . \n",
    "# ----------------------------\n",
    "\n",
    "!git -c safe.directory='*' commit -m \"Day 10\"\n",
    "\n",
    "# Pull just in case there are remote changes\n",
    "!git -c safe.directory='*' pull origin main --no-rebase --no-edit\n",
    "\n",
    "# NOW PUSH\n",
    "!git -c safe.directory='*' push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dd8f2ce-ba72-4c97-9e5b-f9e00b55e123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Day 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0168dfdb-b6b1-470e-9382-708751a0a0bc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Descriptive stats and feature engineering"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 1. Descriptive stats\n",
    "print(\"--- Price Stats ---\")\n",
    "events.describe([\"price\"]).show()\n",
    "\n",
    "# 2. Hypothesis: weekday vs weekend conversion\n",
    "# FIX: Changed 'event_date' to 'event_time'\n",
    "weekday = events.withColumn(\"is_weekend\", \n",
    "    F.dayofweek(\"event_time\").isin([1,7])) \n",
    "\n",
    "print(\"--- Weekend Activity ---\")\n",
    "weekday.groupBy(\"is_weekend\", \"event_type\").count().show()\n",
    "\n",
    "# 3. Correlation\n",
    "# NOTE: Skipped because 'conversion_rate' is not in the column list you provided.\n",
    "# If you need this, you must calculate conversion_rate per user/product first.\n",
    "# print(events.stat.corr(\"price\", \"conversion_rate\")) \n",
    "\n",
    "# 4. Feature engineering\n",
    "# FIX: Changed 'event_date' to 'event_time'\n",
    "features = events.withColumn(\"hour\", F.hour(\"event_time\")) \\\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"event_time\")) \\\n",
    "    .withColumn(\"price_log\", F.log(F.col(\"price\")+1)) \\\n",
    "    .withColumn(\"time_since_first_view\",\n",
    "        # FIX: Cast both to 'long' to perform subtraction in seconds\n",
    "        F.col(\"event_time\").cast(\"long\") - \n",
    "        F.min(\"event_time\").over(Window.partitionBy(\"user_id\")).cast(\"long\")\n",
    "    )\n",
    "\n",
    "print(\"--- Feature Engineering Sample ---\")\n",
    "features.select(\"user_id\", \"event_time\", \"time_since_first_view\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54e80d77-ce98-4577-a695-c39de317a8f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# # 1. UNDO the last commit (so we can try the process again from a clean state)\n",
    "# # We do this to ensure the commit and pull happen in the right order.\n",
    "# !git -c safe.directory='*' reset --soft HEAD~1\n",
    "\n",
    "# 2. SET PRIVATE IDENTITY\n",
    "os.environ[\"GIT_AUTHOR_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_AUTHOR_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "os.environ[\"GIT_COMMITTER_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_COMMITTER_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "\n",
    "# 3. ASK FOR TOKEN\n",
    "print(\"Paste your GitHub Token below and hit Enter:\")\n",
    "GITHUB_TOKEN = getpass.getpass()\n",
    "\n",
    "# 4. PREPARE URL\n",
    "USERNAME = \"SRMajee\"\n",
    "REPO_NAME = \"DataBricks_Challenge\"\n",
    "auth_url = f\"https://{GITHUB_TOKEN}@github.com/{USERNAME}/{REPO_NAME}.git\"\n",
    "\n",
    "# 5. ADD, COMMIT, PULL, THEN PUSH\n",
    "!git -c safe.directory='*' remote set-url origin {auth_url}\n",
    "\n",
    "# --- FIX: ADD FILES FIRST ---\n",
    "!git -c safe.directory='*' add . \n",
    "# ----------------------------\n",
    "\n",
    "!git -c safe.directory='*' commit -m \"Day 11\"\n",
    "\n",
    "# Pull just in case there are remote changes\n",
    "!git -c safe.directory='*' pull origin main --no-rebase --no-edit\n",
    "\n",
    "# NOW PUSH\n",
    "!git -c safe.directory='*' push origin main"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8993193137345771,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook_1",
   "widgets": {
    "layer": {
     "currentValue": "gold",
     "nuid": "eb09bd37-18e3-4545-af26-e4f2591058b0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "bronze",
      "label": null,
      "name": "layer",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "bronze",
        "silver",
        "gold"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "bronze",
      "label": null,
      "name": "layer",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "bronze",
        "silver",
        "gold"
       ]
      }
     }
    },
    "source_path": {
     "currentValue": "/Volumes/workspace/ecommerce/ecommerce_data/2019-Nov.csv",
     "nuid": "8b82adf5-1bef-4e65-9860-793af4f2dd98",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Volumes/workspace/ecommerce/ecommerce_data/bronze_events",
      "label": null,
      "name": "source_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Volumes/workspace/ecommerce/ecommerce_data/bronze_events",
      "label": null,
      "name": "source_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}