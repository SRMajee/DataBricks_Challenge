{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06188412-f34c-415e-b59c-75c5a8a9abb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96a1f9aa-ec9f-4d51-bfe3-a7e2dabef95d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KAGGLE_USERNAME\"] = \"sumeshmajee\"\n",
    "os.environ[\"KAGGLE_KEY\"] = \"KGAT_e8b80db00821caf2bf9e6a6690d004c4\"\n",
    "\n",
    "print(\"Kaggle credentials configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c469599-22a4-4f08-9b9c-1ccaa92c60b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE SCHEMA IF NOT EXISTS workspace.ecommerce\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58aaab97-a86b-40f5-826b-9c89c75abe89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE VOLUME IF NOT EXISTS workspace.ecommerce.ecommerce_data\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89e6da2b-07df-4576-bec3-910cbbc33bc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "cd /Volumes/workspace/ecommerce/ecommerce_data\n",
    "kaggle datasets download -d mkechinov/ecommerce-behavior-data-from-multi-category-store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42e42527-577c-44e6-a00e-5776b35e92da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "cd /Volumes/workspace/ecommerce/ecommerce_data\n",
    "unzip -o ecommerce-behavior-data-from-multi-category-store.zip\n",
    "ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae177ac5-1444-4580-97a5-1060d28ee67d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "cd /Volumes/workspace/ecommerce/ecommerce_data\n",
    "rm -f ecommerce-behavior-data-from-multi-category-store.zip\n",
    "ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e522143-8468-4070-bbd2-56cc8f10242c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c206650-31cb-4ec1-a9c9-071d4704069c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_n = spark.read.csv(\"/Volumes/workspace/ecommerce/ecommerce_data/2019-Nov.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f664cc80-ef4d-44d3-9164-8845e9fd2326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/Volumes/workspace/ecommerce/ecommerce_data/2019-Oct.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bff3746c-4870-40b2-b89e-65dd888e69b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"October 2019 - Total Events: {df.count():,}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCHEMA:\")\n",
    "print(\"=\"*60)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ebe903b-2b33-46fc-8362-8e866558d08e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE DATA (First 5 rows):\")\n",
    "print(\"=\"*60)\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98f21d15-ef86-4c72-b939-7bc02cbeac71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Day 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "117d8e4f-4db6-4498-afe7-b91eafb9435a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create simple DataFrame\n",
    "data = [(\"iPhone\", 999), (\"Samsung\", 799), (\"MacBook\", 1299)]\n",
    "df = spark.createDataFrame(data, [\"product\", \"price\"])\n",
    "df.show()\n",
    "\n",
    "# Filter expensive products\n",
    "df.filter(df.price > 1000).show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a397f2b-25c1-403a-afa9-8336bac13573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# 1. Enter your token securely when prompted (It won't be saved in the file)\n",
    "print(\"Paste your GitHub Token below and hit Enter:\")\n",
    "GITHUB_TOKEN = getpass.getpass()\n",
    "\n",
    "# --- Everything else stays the same ---\n",
    "USERNAME = \"SRMajee\"\n",
    "REPO_NAME = \"DataBricks_Challenge\"\n",
    "auth_url = f\"https://{GITHUB_TOKEN}@github.com/{USERNAME}/{REPO_NAME}.git\"\n",
    "\n",
    "# 2. Update Remote\n",
    "!git -c safe.directory='*' remote set-url origin {auth_url}\n",
    "\n",
    "# 3. Add files (Now the file doesn't contain the secret!)\n",
    "!git -c safe.directory='*' add .\n",
    "\n",
    "# 4. Commit\n",
    "!git -c safe.directory='*' commit -m \"Secure push using getpass\"\n",
    "\n",
    "# 5. Push\n",
    "!git -c safe.directory='*' push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c3845c6-9e0f-43a7-a419-57632378ffd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Day 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6708ae5-2d56-4597-ab95-921a16322662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "events = spark.read.csv(\"/Volumes/workspace/ecommerce/ecommerce_data/2019-Nov.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "930e9ca0-c767-4b72-a073-39b0754cf3fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Basic operations\n",
    "events.select(\"event_time\", \"product_id\", \"price\").show(10)\n",
    "events.filter(\"price > 100\").count()\n",
    "events.groupBy(\"event_time\").count().show()\n",
    "top_brands = events.groupBy(\"brand\").count().orderBy(\"count\", ascending=False).limit(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d059cad0-6a7b-47d7-917a-0216c0374544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# 1. UNDO the last commit (so we can save it again with the private email)\n",
    "!git -c safe.directory='*' reset --soft HEAD~1\n",
    "\n",
    "# 2. SET PRIVATE IDENTITY (Using your GitHub no-reply alias)\n",
    "# This format (username@users.noreply.github.com) hides your real email\n",
    "os.environ[\"GIT_AUTHOR_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_AUTHOR_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "os.environ[\"GIT_COMMITTER_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_COMMITTER_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "\n",
    "# 3. ASK FOR TOKEN\n",
    "print(\"Paste your GitHub Token below and hit Enter:\")\n",
    "GITHUB_TOKEN = getpass.getpass()\n",
    "\n",
    "# 4. PREPARE URL\n",
    "USERNAME = \"SRMajee\"\n",
    "REPO_NAME = \"DataBricks_Challenge\"\n",
    "auth_url = f\"https://{GITHUB_TOKEN}@github.com/{USERNAME}/{REPO_NAME}.git\"\n",
    "\n",
    "# 5. COMMIT & PUSH\n",
    "!git -c safe.directory='*' remote set-url origin {auth_url}\n",
    "!git -c safe.directory='*' commit -m \"Day 2\"\n",
    "!git -c safe.directory='*' push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1525c337-4b41-4933-9454-186ae49c5c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Day 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caccce9a-00b5-43f7-a860-619a5c794da0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Top 5 products by revenue\n",
    "revenue = (\n",
    "    events.filter(F.col(\"event_type\") == \"purchase\")\n",
    "    .groupBy(\"product_id\", \"product_id\")\n",
    "    .agg(F.sum(\"price\").alias(\"revenue\"))\n",
    "    .orderBy(F.desc(\"revenue\"))\n",
    "    .limit(5)\n",
    ")\n",
    "\n",
    "# Running total per user\n",
    "window = Window.partitionBy(\"user_id\").orderBy(\"event_time\")\n",
    "events_with_cumulative = events.withColumn(\n",
    "    \"cumulative_events\",\n",
    "    F.count(\"*\").over(window)\n",
    ")\n",
    "\n",
    "# Conversion rate by category (replace pivot with conditional aggregation)\n",
    "conversion = (\n",
    "    events.groupBy(\"category_code\")\n",
    "    .agg(\n",
    "        F.sum(F.when(F.col(\"event_type\") == \"purchase\", 1).otherwise(0)).alias(\"purchase\"),\n",
    "        F.sum(F.when(F.col(\"event_type\") == \"view\", 1).otherwise(0)).alias(\"view\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"conversion_rate\",\n",
    "        F.col(\"purchase\") / F.col(\"view\") * 100\n",
    "    )\n",
    ")\n",
    "\n",
    "display(revenue)\n",
    "display(events_with_cumulative)\n",
    "display(conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f749ee-cf05-45c4-a2e5-5d73cd6b2edd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# 1. UNDO the last commit (so we can try the process again from a clean state)\n",
    "# We do this to ensure the commit and pull happen in the right order.\n",
    "!git -c safe.directory='*' reset --soft HEAD~1\n",
    "\n",
    "# 2. SET PRIVATE IDENTITY\n",
    "os.environ[\"GIT_AUTHOR_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_AUTHOR_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "os.environ[\"GIT_COMMITTER_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_COMMITTER_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "\n",
    "# 3. ASK FOR TOKEN\n",
    "print(\"Paste your GitHub Token below and hit Enter:\")\n",
    "GITHUB_TOKEN = getpass.getpass()\n",
    "\n",
    "# 4. PREPARE URL\n",
    "USERNAME = \"SRMajee\"\n",
    "REPO_NAME = \"DataBricks_Challenge\"\n",
    "auth_url = f\"https://{GITHUB_TOKEN}@github.com/{USERNAME}/{REPO_NAME}.git\"\n",
    "\n",
    "# 5. COMMIT, PULL (WITH FIX), THEN PUSH\n",
    "!git -c safe.directory='*' remote set-url origin {auth_url}\n",
    "!git -c safe.directory='*' commit -m \"Day 3\"\n",
    "\n",
    "# FIX IS HERE: We added '--no-rebase' to tell Git to use the default merge strategy\n",
    "!git -c safe.directory='*' pull origin main --no-rebase --no-edit\n",
    "\n",
    "# NOW PUSH\n",
    "!git -c safe.directory='*' push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69a944f8-3241-4e68-b347-eb5fe6511805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Day 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebe6e286-6250-47c1-87d9-0469d9b1419f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write Delta table and schema note"
    }
   },
   "outputs": [],
   "source": [
    "# Use a Unity Catalog volume directory path, not a file\n",
    "volume_path = \"/Volumes/workspace/ecommerce/ecommerce_data/events_delta\"\n",
    "\n",
    "# Write DataFrame as Delta table to the directory\n",
    "events.write.format(\"delta\").mode(\"overwrite\").save(volume_path)\n",
    "\n",
    "# Register the Delta table in Unity Catalog\n",
    "events.write.format(\"delta\").saveAsTable(\"workspace.ecommerce.events_table\")\n",
    "\n",
    "# SQL approach to create a managed Delta table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE workspace.ecommerce.events_delta\n",
    "    USING DELTA\n",
    "    AS SELECT * FROM workspace.ecommerce.events_table\n",
    "\"\"\")\n",
    "\n",
    "# Test schema enforcement\n",
    "try:\n",
    "    wrong_schema = spark.createDataFrame([(\"a\",\"b\",\"c\")], [\"x\",\"y\",\"z\"])\n",
    "    wrong_schema.write.format(\"delta\").mode(\"append\").save(volume_path)\n",
    "except Exception as e:\n",
    "    print(f\"Schema enforcement: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df9ea388-4a94-4f08-ba59-501400ba2637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# # 1. UNDO the last commit (so we can try the process again from a clean state)\n",
    "# # We do this to ensure the commit and pull happen in the right order.\n",
    "# !git -c safe.directory='*' reset --soft HEAD~1\n",
    "\n",
    "# 2. SET PRIVATE IDENTITY\n",
    "os.environ[\"GIT_AUTHOR_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_AUTHOR_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "os.environ[\"GIT_COMMITTER_NAME\"] = \"Sumesh Majee\"\n",
    "os.environ[\"GIT_COMMITTER_EMAIL\"] = \"SRMajee@users.noreply.github.com\"\n",
    "\n",
    "# 3. ASK FOR TOKEN\n",
    "print(\"Paste your GitHub Token below and hit Enter:\")\n",
    "GITHUB_TOKEN = getpass.getpass()\n",
    "\n",
    "# 4. PREPARE URL\n",
    "USERNAME = \"SRMajee\"\n",
    "REPO_NAME = \"DataBricks_Challenge\"\n",
    "auth_url = f\"https://{GITHUB_TOKEN}@github.com/{USERNAME}/{REPO_NAME}.git\"\n",
    "\n",
    "# 5. ADD, COMMIT, PULL, THEN PUSH\n",
    "!git -c safe.directory='*' remote set-url origin {auth_url}\n",
    "\n",
    "# --- FIX: ADD FILES FIRST ---\n",
    "!git -c safe.directory='*' add . \n",
    "# ----------------------------\n",
    "\n",
    "!git -c safe.directory='*' commit -m \"Day 4\"\n",
    "\n",
    "# Pull just in case there are remote changes\n",
    "!git -c safe.directory='*' pull origin main --no-rebase --no-edit\n",
    "\n",
    "# NOW PUSH\n",
    "!git -c safe.directory='*' push origin main"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}